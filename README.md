摘要
     随着互联网技术的不断发展，互联网信息呈爆炸式增长，同时互联网用户对信息的需求也在不断增长。在巨大的用户需求中，搜索引擎应运而生。搜素引擎已经改变了信息的获取甚至存储方式。用户不在需要将大量信息存储在终端机上，而是在需要信息的时候直接运用搜索引擎来获取，这样不仅节约了存储空间，也能获取到最新，最准确的数据。
网络爬虫的主要作用是获取互联网上的信息。其实现的基本功能包括下载网页以及对URL地址的遍历。为了高效的快速遍历网站还需要应用专门的数据结构来优化。
本文以DotnetSpider程序作为研究对象，分析了DotnetSpider的运作机制，并对该程序做了改进和扩展。
第一章 简介
1.1概述
    网络爬虫，又称为网络蜘蛛或Web信息采集器，是一个自动下载网页的计算机程序或自动化脚本，是搜索引擎的重要组成部分。网络爬虫通常从一个称为种子集的URL集合开始运行，它首先将这些URL全部放入到一个有序的待爬行队列里，按照一定的顺序从中取出URL并下载所指向的页面，分析页面内容，提取新的URL并存入待爬行URL队列中，如此重复上面的过程，知道URL为空或满足某个爬行终止条件，从而遍历Web。该过程称为网络爬行。
1.2问题的陈述和研究变量的识别
    为了提升爬行效率，爬虫需要在单位时间内尽可能多的获取高质量页面。
1.3问题的重要性
截止到2007年底，Internet上网页数量超过160亿个，研究表明接近30%的页面是重复的；动态页面的存在：客户端，服务器端脚本语言的应用使得指向相同Web信息的URL数量呈指数级增长。上述特征使得网络爬虫面临一定的困难，主要体现在Web信息的巨大容量使得爬虫在给定时间只能下载少量网页。
为提高爬行效率，爬虫需要在单位时间内尽可能多的获取高质量页面，是它面临的难题之一。当前有五种表示页面质量高低的方式：Similarity（页面与爬行主体之间的相似度），Backlink（页面在Web图中的入度大小），PageRank（指向它的所有页面的平均权值），Forwardlink（页面在Web图中的出度大小），Location（页面的信息位置），Parallel（并行性问题）。为了提高爬行速度，网络通常会采取并行爬行的工作方式，随之引入了新的问题：重复性（并行运行的爬虫或爬行线程同时增加了重复页面），质量问题（并行运行时，每个爬虫和爬虫线程之间不可避免要进行一些通信）。并行运行时，网络爬虫通常采用三种方式：独立方式（各个爬虫独立爬行页面，互不通信），动态分配方式（由一个中央协调器动态协调分配URL给各个爬虫），静态分配方式（URL事先划分给各个爬虫）。
1.4研究目标
     本文旨在分析给定的DotnetSpider爬虫程序，并在已有的基础上对程序进行改进和扩展。
1.5定义
    网络爬虫，又称为网络蜘蛛或Web信息采集器，是一个自动下载网页的计算机程序或自动化脚本，是搜索引擎的重要组成部分。
研究的假设和局限性
    
1.6研究的理论框架
一般的网络爬虫软件都由以下几个模块组成：
（1）保存种子和爬取出来的URL的数据结构。通常爬虫都是从一系列的种子URL开始爬取，一般从数据库表或者配置文件中读取这些种子URL。而保存待抓取的URL的数据结构却会因为系统的规模，功能不同有可能采取不同的策略。
（2）保存已经抓取过的URL的数据结构。已经抓取过的URL规模和待抓取的URL的规模是一个相当的量级。
（3）页面获取模块。当从种子URL队列或者抓取出来的URL队列中获得URL后，便要根据这个URL来获得当前页面的内容，获得的方法非常简单，就是普通的I/O操作。在这个模块中，仅仅是把URL所指的内容按照二进制的格式读出来，而不对内容做任何处理。
（4）提取已经获得的网页的内容中的有效信息。从页面获取模块的结果是一个表示HTML源代码的字符串。从这个字符串中抽取各种相关的内容，是爬虫软件的目的。
（5）负责连接前处理模块，负责连接后处理模块和过滤器模块。如果只抓取某个网站的网页，则可以对URL按域名过滤。
（6）多线程模块。爬虫主要消耗三种资源：网络宽带，中央处理器和磁盘。三者中任何一者都有可能成为瓶颈，其中网络宽带一般是租用，价格相对昂贵。为了增加爬虫效率，
第二章 研究方法
2.1研究设计的确定
从网站上爬取数据首先是从一个网页下载程序，只要用WebClient/WebRequest打开Url地址，将数据流存入本地存储器的文件即可。然后如何获得更多的网页呢？这里可以分为两步，第一步是得到更多的地址，第二步是下载地址指向的链接内容。重复执行以上两步就可以完成爬取网页数据的功能。想要得到更多的地址，最好的办法是模拟人使用网页的办法。人们怎么浏览网站呢？当然是从主页依次点开各层的链接。分析已经下载的主页文本，提取其中所有的Url地址信息，再依次下载得到的Url地址指向的链接。现在网络上有不少Web2.0的网站了，这对解析提供了很大的方便。
链接可以分为两类：1.完整链接，即：“http：//www.163.com”类，前面有明显的标志http：//，这样的内容很好提取，只要String的静态方法IndexOf（）找到http：//的位置以及从此位置算起第一个“””出现的位置（即链接结束的位置），再用SubString（）方法将地址提取出来即可。2.  非完整链接，其形式一般为”\index.htm”，提取方法和完整链接的方法相同，只是判断它是不是链接上有一定难度（因为属性等其它信息也可能以“”\”开头，这时就很难判断了。笔者采取的方法是试下载，即下载一下试试，如果超时则不是，不超时则是。注意：要在它的前面加上根地址，如“http://www.163.com/index.htm”。
上述方法已经可以实现爬虫的功能了，但是它有着效率低下的问题，下载速度可能会很慢。这有两方面的原因：
1. 分析和下载不能同步进行。在单线程的程序中，两者是无法同时进行的。也就是说，分析时会造成网络空闲，分析的时间越长，下载的效率越低。反之也是一样，下载时无法同时进行分析，只有停下下载后才能进行下一步的分析。问题浮出水面，我想大家都会想到：把分析和下载用不同的线程进行，问题不就解决了吗？
2.只是单线程下载。相信大家都有用过网际快车等下载资源的经历，它里面是可以设置线程数的（近年版本默认是10，曾经默认是5）。它会将文件分成与线程数相同的部分，然后每个线程下载自己的那一部分，这样下载效率就有可能提高。相信大家都有加多线程数，提升下载效率的经历。但细心的用户会发现，在带宽一定的情况下，并不是线程越多，速度越快，而是在某一点达到峰值。爬虫作为特殊的下载工具，不具备多线程的能力何以有效率可谈？爬虫在信息时代的目的，难道不是快速获取信息吗？所以，爬虫需要有多线程（可控数量）同时下载网页。
多线程在C#中并不难实现。它有一个命名空间：System.Threading，提供了多线程的支持。
由于线程起始时启动的方法不能带有参数，这就为多线程共享资源添加了麻烦。不过我们可以用类级变量（当然也可以使用其它方法，笔者认为此方法最简单易用）来解决这个问题。知道开启多线程下载的方法后，大家可能会产生几个疑问：
1.如何控制线程的数量？
2.如何防止多线程下载同一网页？
3.如何判断线程结束？
4.如何控制线程结束？
下面就这几个问题提出解决方法：
1.线程数量我们可以通过for循环来实现。
2.下面出现的一个问题：所有的线程都调用DonwLoad()方法，这样如何避免它们同时下载同一个网页呢？这个问题也好解决，只要建立一下Url地址表，表中的每个地址只允许被一个线程申请即可。具体实现：  
可以利用数据库，建立一个表，表中有四列，其中一列专门用于存储Url地址，另外两列分别存放地址对应的线程以及该地址被申请的次数，最后一列存放下载的内容。（当然，对应线程一列不是必要的）。当有线程申请后，将对应线程一列设定为当前线程编号，并将是否申请过一列设置为申请一次，这样，别的线程就无法申请该页。如果下载成功，则将内容存入内容列。如果不成功，内容列仍为空，作为是否再次下载的依据之一，如果反复不成功，则进程将于达到重试次数（对应该地址被申请的次数，用户可设）后，申请下一个Url地址。
3.线程结束是很难判断的，因为它总是在查找新的链接。用者认为可以假设：线程重复N次以后还是没有能申请到新的Url地址，那么可以认为它已经下载完了所有链接。
4.这个问题相对简单，因为在问题一中已经建议，将线程声名为类级数组，这样就很易于控制。只要用一个for循环即可结束。

2.2研究设计（包括操作定义）
2.2.1程序分析
从主程序分析，程序首先实例化了一个Spider1，该类继承自CustomSpider类，一个基本的爬虫类，包含有Run方法，PrepareDb()，InsertTask()，InsertBatch()方法，后三个方法用来准备数据库和插入命令。
实例化之后运行了Run方法。
实例化DDengEntitySpider，是EntitySpiderBuilder的子类，其中的Run方法会对ddeng.com网站进行爬取。
Cnblogs是对http://www.cnblogs.com程序进行爬取的类。site定义要采集的 Site 对象, 可以设置 Header、Cookie、代理等。AddStartUrl方法添加初始采集链接。Spider.Create使用内存Scheduler、自定义PageProcessor、自定义Pipeline创建爬虫。spider.Run()启动爬虫。
MyPipeline是一个管道类，它是BasePipeline的子类，可以自由实现插入数据库或保存到文件。
BlogSumaryProcessor是一个处理器类，它是BasePageProcessor的子类，BlogSumaryProcessor()定义目标页的筛选，Handle（）利用Selectable查询并构造自己想要的数据对象，AddResultItem（）方法以自定义KEY存入page对象中供Pipeline调用。NewsProcessor与上述类同。
CasSpider，BaiduSpider，JdskuSampleSpider，Situoli类分别是爬取Cas网站，Baidu网站，Jd网站以及Situoli的类，方法与DDengSpider，Cnblogs相同。

2.2.2程序的改进
   本文对程序的改进详情可以查看网址。
2.2.3 程序的扩展
该小节实现了对爬取校内通知的扩充，扩充的程序为Sample.JluSpider类，可以登录网址查看。
2.3数据收集
     评估模型的制定：需要一些指标来衡量不同并行网络爬虫机制的优劣性，这些指标作为后边实验的评估依据。
    （1）重叠度
当多个网络爬虫进程同时下载到一个相同的页面时，这个页面就有可能被多个网络爬虫下载到多次，这就是重叠。设计者应该在爬虫架构设计中避免重叠度太高的情况。
为了更加精确，定义了页面下载重叠度的指标为（N-I）/I。N代表从全局来看所有网络爬虫进程下载到的页面总数，I代表被下载到的没有重复的网络页面的个数。因此，整个公式的意义就是网络爬虫进程下载到的无效页面和有效页面的比值，优化网络爬虫程序的目标是减少这个指标。
（2）覆盖率
当多个网络爬虫进程各自运行时，爬虫进程可能不会完全下载到本属于它职责范围内的所有页面。可以把这个问题公式化：先把网络爬虫程序下载网络页面的覆盖率定位I/U。U代表网络爬虫进程应该去下载的网络页面个数，I代表网络爬虫进程间也不存在外快链接的交换。注意，这个值一定要是去除掉重叠的独一无二的有效页面。
（3）质量
有的时候，网络爬虫程序不需要下载整个网站，而只是想下载一些重要性比较强的网页或者这个相关的一些内容。例如，一个网络爬虫程序的存储空间只够容纳1000000的页面，所以，它只想从网站中下载最重要的前1000000个页面。为了实现这个机制，网络爬虫程序必须能够分辨哪些页面是重要的网页，称之为重要性指标。
首先，假设一个爬虫程序已经可以通过一个极为精确的方法知道一个要被下载页面的重要性。网络爬虫程序要下载N个重要的页面，PN代表这N个页面的集合。同时，用AN代表这N个页面中实际被爬虫下载到的个数。然后定义|AN并PN|/|PN|作为网络爬虫下载网络页面质量的公式。在这个公式里表示PN集合里边有多少个页面是真正被下载了。
（4）进程通讯开销
并行网络爬虫技术需要在不同的进程间交换信息以协调它们之间的工作。在交换模式下，网络爬虫进程间定期的交换它们的块外链接。为了定量的计算交换需要多少通信开销，定义了通讯开销指标作为每个网络页面需要交换的块外链接平均值的衡量标准。
第三章 数据分析与结果
3.1假设检验
通过运行程序，本文对于DotnetSpider给出以下评价：
程序的覆盖率为好，程序的重叠度为好，质量为差，通讯开销为好。
3.2研究中的有效性
    程序能够有效运行，可以对Sample主程序的DDeng网站，Cnblogs网站，cas网站，吉林大学校内通知网站，百度网站，jd网站进行爬取操作。
3.3可靠性	
   从程序选择爬取的网站来看，DotnetSpider程序具有一定的可靠性。
第四章 总结
   本文首先介绍了网络爬虫的概念，自身存在的问题。然后本文阐述了研究的目标和本文的理论研究框架，即爬虫软件的模块组成。在研究方法中，本文具体介绍了如何构建一个网络爬虫程序。然后本文阐述了对于DotnetSpider程序的分析，改进与扩展。本文在源程序的基础上扩展了一个爬取校内通知的内容。在第三章的最后，本文给出了评定爬虫程序性能的标准。在第三章，本文对程序运行得出的数据做了分析。
